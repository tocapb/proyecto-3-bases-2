{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripción del sistema\n",
    "El sistema consiste en el desarrollo de 5 gráficos diferentes, estos gráficos son realizados con dos conjuntos de datos de dos entidades costarricenses diferentes que son el INEC y el OIJ, ambos conjuntos de datos fueron descargados en formato csv y son almacenados como dataframes utilizando la función de spark en python, además de estos los datos son limpiados ya que tienen incongruencias en su formato, una vez con los datos limpios se envían los mismo a postgres y se grafican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas necesarias para el uso de spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.files import SparkFiles\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se guardan ambos archivos como dataframes\n",
    "oij_csv=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"Estadisticas.csv\")\n",
    "inec_csv=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"reempleocenso2011-22.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Delito: string (nullable = true)\n",
      " |-- SubDelito: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- Hora: string (nullable = true)\n",
      " |-- Victima: string (nullable = true)\n",
      " |-- SubVictima: string (nullable = true)\n",
      " |-- Edad: string (nullable = true)\n",
      " |-- Genero: string (nullable = true)\n",
      " |-- Nacionalidad: string (nullable = true)\n",
      " |-- Provincia: string (nullable = true)\n",
      " |-- Canton: string (nullable = true)\n",
      " |-- Distrito: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Se imprimen ambos esquemas para observar el formato en el que vienen los datos, esto servirá para ajustar\n",
    "#el dataframe a lo que necesitemos\n",
    "#este de abajo será el formato del OIJ\n",
    "oij_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProvinciaCantonDistrito: string (nullable = true)\n",
      " |-- PoblacionMayor15: integer (nullable = true)\n",
      " |-- TasaNetaParticipacion: double (nullable = true)\n",
      " |-- TasaOcupacion: double (nullable = true)\n",
      " |-- TasaDesempleoAbierto: string (nullable = true)\n",
      " |-- PorcentajeEconomicamenteInactivo: double (nullable = true)\n",
      " |-- RelacionDependenciaEconomica: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Este otro formato correspone al INEC\n",
    "inec_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se registran ambos dataframe como tablas de SQL, esto nos permite realizar consultas sql por medio de sqlContext\n",
    "sqlContext.registerDataFrameAsTable(oij_csv, \"oij\")\n",
    "sqlContext.registerDataFrameAsTable(inec_csv, \"inec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Entradas: Un entero\n",
    "Salidas: Un dataframe\n",
    "Descripción general: Se encarga de validar cual conjunto de datos se debe enviar \n",
    "y realiza la consulta SQL correspondiente la cual removerá espacios en blanco\n",
    "'''\n",
    "def quitaEspacios(ind):\n",
    "    #Este if es de validación para enviar datos de INEC o del OIJ\n",
    "    #por medio de sqlContext se utiliza la función TRIM que acorta todos los espacios en blanco\n",
    "    #almacenados antes o después de nuestros datos\n",
    "    if ind == 1:\n",
    "        return sqlContext.sql(\"SELECT Delito, SubDelito,Hora, Fecha, Victima, SubVictima, Edad, Genero, Nacionalidad, Provincia,Canton, TRIM(Distrito) as Distrito from oij\")\n",
    "    else:\n",
    "        return sqlContext.sql(\"SELECT TRIM(ProvinciaCantonDistrito) AS ProvinciaCantonDistrito, PoblacionMayor15, TasaNetaParticipacion, TasaOcupacion, TasaDesempleoAbierto, PorcentajeEconomicamenteInactivo, RelacionDependenciaEconomica from inec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplemente se llama a la función con el número entero correspondiente y se registra el nuevo dataframe\n",
    "#como la tabla almacenada anteriormente en el sistema, esto se aplica para ambos conjuntos de datos\n",
    "oij = quitaEspacios(1)\n",
    "sqlContext.registerDataFrameAsTable(oij, \"oij\")\n",
    "inec = quitaEspacios(2)\n",
    "sqlContext.registerDataFrameAsTable(inec, \"inec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Entradas: Un entero\n",
    "Salidas: Un dataframe\n",
    "Descripción general: Se encarga de validar cual conjunto de datos se debe enviar \n",
    "y realiza la consulta SQL correspondiente la cual pasará los datos a minúscula\n",
    "'''\n",
    "def minusculas(ind):\n",
    "    #Este if es de validación para enviar datos de INEC o del OIJ\n",
    "    #por medio de sqlContext se utiliza la función LOWER que se encarga de pasar los datos a minúscula\n",
    "    if ind == 1:\n",
    "        return sqlContext.sql(\"SELECT Delito, SubDelito,Hora, Fecha, Victima, SubVictima, Edad, Genero, Nacionalidad, Provincia,Canton, LOWER(Distrito) as Distrito from oij\")\n",
    "    else:\n",
    "        return sqlContext.sql(\"SELECT LOWER(ProvinciaCantonDistrito) AS ProvinciaCantonDistrito, PoblacionMayor15, TasaNetaParticipacion, TasaOcupacion, TasaDesempleoAbierto, PorcentajeEconomicamenteInactivo, RelacionDependenciaEconomica from inec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se llama a la función con el número entero correspondiente y se registra el nuevo dataframe\n",
    "#como la tabla almacenada anteriormente en el sistema, se aplica a ambos conjuntos de datos\n",
    "oij = minusculas(1)\n",
    "sqlContext.registerDataFrameAsTable(oij, \"oij\")\n",
    "inec = minusculas(2)\n",
    "sqlContext.registerDataFrameAsTable(inec, \"inec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Entradas: No posee\n",
    "Salidas: Un dataframe\n",
    "Descripción general: Se encarga de hacer la consulta sql para mostrar los datos del OIJ\n",
    "que no coinciden con ningún dato del INEC\n",
    "'''\n",
    "def sacaNoExistentes():\n",
    "    #Esta consulta se encarga de sacar los distintos por medio del uso de DISTINCT que no se encuentren\n",
    "    #en la consulta realizada a los datos del INEC\n",
    "    return sqlContext.sql(\"SELECT DISTINCT(Distrito) FROM oij WHERE NOT EXISTS(SELECT 1 FROM inec WHERE inec.ProvinciaCantonDistrito = oij.Distrito)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Llama a la función y lo almacena como un dataframe nuevo y una tabla nueva\n",
    "noCoinciden = sacaNoExistentes()\n",
    "sqlContext.registerDataFrameAsTable(noCoinciden, \"noCoincidencias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|     Distrito|\n",
      "+-------------+\n",
      "|       pococi|\n",
      "|      guacimo|\n",
      "|     la union|\n",
      "|         null|\n",
      "|  leon cortes|\n",
      "|     san jose|\n",
      "|    san ramon|\n",
      "|        canas|\n",
      "|        limon|\n",
      "|perez zeledon|\n",
      "|        tibas|\n",
      "|    sarapiqui|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Se muestran los datos que no coincidieron, principalmente fue porque el OIJ utiliza abreviaciones y no usa tildes,\n",
    "#el INEC no\n",
    "noCoinciden.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Entradas: No posee\n",
    "Salidas: Un dataframe\n",
    "Descripción general: Se encarga de contar los elementos de la tabla de noCoincidencias\n",
    "estos elementos corresponen a todos aquellos distritos del OIJ que no corresponden a distritos del INEC\n",
    "'''\n",
    "def cuentaNoExistentes():\n",
    "    return sqlContext.sql(\"SELECT COUNT(*) as num FROM noCoincidencias\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Almacena el dataframe con el dato\n",
    "numNoCoinciden = cuentaNoExistentes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "| 12|\n",
      "+---+\n",
      "\n",
      "Cantidad de registros que no coincidieron:  12\n"
     ]
    }
   ],
   "source": [
    "#Se despliega el número correspondiente\n",
    "numNoCoinciden.show()\n",
    "print(\"Cantidad de registros que no coincidieron: \", noCoinciden.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se modifican datos del dataframe del INEC para que correspondan a algunos datos del OIJ\n",
    "#esto se encarga de buscar por columna los datos que tengan almaceanado los nombres de la manera indicada\n",
    "#luego se modifican por sus correpondientes en el dataframe del OIJ\n",
    "inec = inec.withColumn(\"ProvinciaCantonDistrito\", F.when(F.col(\"ProvinciaCantonDistrito\")=='pococí','pococi').otherwise(F.col(\"ProvinciaCantonDistrito\")))\n",
    "inec = inec.withColumn(\"ProvinciaCantonDistrito\", F.when(F.col(\"ProvinciaCantonDistrito\")=='la unión','la union').otherwise(F.col(\"ProvinciaCantonDistrito\")))\n",
    "inec = inec.withColumn(\"ProvinciaCantonDistrito\", F.when(F.col(\"ProvinciaCantonDistrito\")=='belén','belen').otherwise(F.col(\"ProvinciaCantonDistrito\")))\n",
    "inec = inec.withColumn(\"ProvinciaCantonDistrito\", F.when(F.col(\"ProvinciaCantonDistrito\")=='león cortés castro','leon cortes').otherwise(F.col(\"ProvinciaCantonDistrito\")))\n",
    "inec = inec.withColumn(\"ProvinciaCantonDistrito\", F.when(F.col(\"ProvinciaCantonDistrito\")=='san josé','san jose').otherwise(F.col(\"ProvinciaCantonDistrito\")))\n",
    "#se guarda el dataframe modificado como una tabla\n",
    "sqlContext.registerDataFrameAsTable(inec, \"inec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se integran por medio de un inner join los conjuntos de datos,y se almacenan como un nuevo dataframe\n",
    "#este dataframe será enviado a postgres\n",
    "datosIntegrados = sqlContext.sql(\"SELECT a.Delito, a.SubDelito, a.Hora, a.Fecha, a.Victima, a.SubVictima, a.Edad, a.Genero, a.Nacionalidad, a.Provincia, a.Canton, a.Distrito, b.PoblacionMayor15, b.TasaNetaParticipacion, b.TasaOcupacion, b.TasaDesempleoAbierto, b.PorcentajeEconomicamenteInactivo, b.RelacionDependenciaEconomica FROM oij a INNER JOIN inec b ON a.Distrito = b.ProvinciaCantonDistrito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Delito: string (nullable = true)\n",
      " |-- SubDelito: string (nullable = true)\n",
      " |-- Hora: string (nullable = true)\n",
      " |-- Fecha: string (nullable = true)\n",
      " |-- Victima: string (nullable = true)\n",
      " |-- SubVictima: string (nullable = true)\n",
      " |-- Edad: string (nullable = true)\n",
      " |-- Genero: string (nullable = true)\n",
      " |-- Nacionalidad: string (nullable = true)\n",
      " |-- Provincia: string (nullable = true)\n",
      " |-- Canton: string (nullable = true)\n",
      " |-- Distrito: string (nullable = true)\n",
      " |-- PoblacionMayor15: integer (nullable = true)\n",
      " |-- TasaNetaParticipacion: double (nullable = true)\n",
      " |-- TasaOcupacion: double (nullable = true)\n",
      " |-- TasaDesempleoAbierto: string (nullable = true)\n",
      " |-- PorcentajeEconomicamenteInactivo: double (nullable = true)\n",
      " |-- RelacionDependenciaEconomica: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datosIntegrados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datosIntegrados=datosIntegrados.withColumn('TasaDesempleoAbierto',datosIntegrados['TasaDesempleoAbierto'].cast(\"float\").alias('TasaDesempleoAbierto'))\n",
    "datosIntegrados=datosIntegrados.withColumn('TasaNetaParticipacion',datosIntegrados['TasaNetaParticipacion'].cast(\"float\").alias('TasaNetaParticipacion'))\n",
    "datosIntegrados=datosIntegrados.withColumn('TasaOcupacion',datosIntegrados['TasaOcupacion'].cast(\"float\").alias('TasaOcupacion'))\n",
    "datosIntegrados=datosIntegrados.withColumn('PorcentajeEconomicamenteInactivo',datosIntegrados['PorcentajeEconomicamenteInactivo'].cast(\"float\").alias('PorcentajeEconomicamenteInactivo'))\n",
    "datosIntegrados=datosIntegrados.withColumn('RelacionDependenciaEconomica',datosIntegrados['RelacionDependenciaEconomica'].cast(\"float\").alias('RelacionDependenciaEconomica'))\n",
    "datosIntegrados=datosIntegrados.withColumn('Fecha',to_date(unix_timestamp(col('Fecha'), 'yyyy-MM-dd').cast(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Delito: string (nullable = true)\n",
      " |-- SubDelito: string (nullable = true)\n",
      " |-- Hora: string (nullable = true)\n",
      " |-- Fecha: date (nullable = true)\n",
      " |-- Victima: string (nullable = true)\n",
      " |-- SubVictima: string (nullable = true)\n",
      " |-- Edad: string (nullable = true)\n",
      " |-- Genero: string (nullable = true)\n",
      " |-- Nacionalidad: string (nullable = true)\n",
      " |-- Provincia: string (nullable = true)\n",
      " |-- Canton: string (nullable = true)\n",
      " |-- Distrito: string (nullable = true)\n",
      " |-- PoblacionMayor15: integer (nullable = true)\n",
      " |-- TasaNetaParticipacion: float (nullable = true)\n",
      " |-- TasaOcupacion: float (nullable = true)\n",
      " |-- TasaDesempleoAbierto: float (nullable = true)\n",
      " |-- PorcentajeEconomicamenteInactivo: float (nullable = true)\n",
      " |-- RelacionDependenciaEconomica: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datosIntegrados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------------+----------+-----------+----------+--------------------+-------------+------------+----------+----------+-------------+----------------+---------------------+-------------+--------------------+--------------------------------+----------------------------+\n",
      "|Delito|    SubDelito|               Hora|     Fecha|    Victima|SubVictima|                Edad|       Genero|Nacionalidad| Provincia|    Canton|     Distrito|PoblacionMayor15|TasaNetaParticipacion|TasaOcupacion|TasaDesempleoAbierto|PorcentajeEconomicamenteInactivo|RelacionDependenciaEconomica|\n",
      "+------+-------------+-------------------+----------+-----------+----------+--------------------+-------------+------------+----------+----------+-------------+----------------+---------------------+-------------+--------------------+--------------------------------+----------------------------+\n",
      "|ASALTO|  ARMA BLANCA|18:00:00 - 20:59:59|2011-09-19|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|  ALAJUELA|     alajuela|           34485|                 55.1|         53.4|                 3.1|                            44.9|                         1.3|\n",
      "|ASALTO|  ARMA BLANCA|18:00:00 - 20:59:59|2011-09-19|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|  ALAJUELA|     alajuela|          195003|                 55.8|         53.9|                 3.5|                            44.2|                         1.3|\n",
      "|ASALTO|  ARMA BLANCA|18:00:00 - 20:59:59|2011-09-19|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|  ALAJUELA|     alajuela|          632572|                 52.9|         51.3|                 3.0|                            47.1|                         1.5|\n",
      "|ASALTO|ARMA DE FUEGO|18:00:00 - 20:59:59|2011-03-17|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|   CARTAGO|     oreamuno|           34110|                 53.9|         52.4|                 2.8|                            46.1|                         1.5|\n",
      "|ASALTO|ARMA DE FUEGO|21:00:00 - 23:59:59|2011-06-30|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE| NICARAGUA|  SAN JOSE| desamparados|           19768|                 57.1|         55.0|                 3.7|                            42.9|                         1.3|\n",
      "|ASALTO|ARMA DE FUEGO|21:00:00 - 23:59:59|2011-06-30|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE| NICARAGUA|  SAN JOSE| desamparados|           26951|                 59.3|         56.4|                 4.8|                            40.7|                         1.1|\n",
      "|ASALTO|ARMA DE FUEGO|21:00:00 - 23:59:59|2011-06-30|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE| NICARAGUA|  SAN JOSE| desamparados|          159292|                 57.0|         54.7|                 4.0|                            43.0|                         1.3|\n",
      "|ASALTO|ARMA DE FUEGO|12:00:00 - 14:59:59|2011-08-10|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|  SAN JOSE|montes de oca|           41561|                 58.8|         57.1|                 2.9|                            41.2|                         1.0|\n",
      "|ASALTO|ARMA DE FUEGO|12:00:00 - 14:59:59|2011-06-28|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|   CARTAGO|     la union|             968|                 41.8|         41.2|                 1.5|                            58.2|                         2.1|\n",
      "|ASALTO|ARMA DE FUEGO|12:00:00 - 14:59:59|2011-06-28|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|   CARTAGO|     la union|           75029|                 59.3|         57.3|                 3.4|                            40.7|                         1.2|\n",
      "|ASALTO|ARMA DE FUEGO|18:00:00 - 20:59:59|2011-07-30|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|PUNTARENAS|   puntarenas|            6801|                 53.3|         51.4|                 3.4|                            46.7|                         1.3|\n",
      "|ASALTO|ARMA DE FUEGO|18:00:00 - 20:59:59|2011-07-30|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|PUNTARENAS|   puntarenas|           84210|                 48.2|         46.4|                 3.7|                            51.8|                         1.8|\n",
      "|ASALTO|ARMA DE FUEGO|18:00:00 - 20:59:59|2011-07-30|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|PUNTARENAS|   puntarenas|          295918|                 48.9|         47.3|                 3.3|                            51.1|                         1.8|\n",
      "|ASALTO|ARMA DE FUEGO|15:00:00 - 17:59:59|2011-01-12|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE|COSTA RICA|  SAN JOSE|   goicoechea|           90537|                 56.8|         54.7|                 3.7|                            43.2|                         1.2|\n",
      "|ASALTO|ARMA DE FUEGO|21:00:00 - 23:59:59|2011-11-02|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE| NICARAGUA|   HEREDIA|      heredia|           16071|                 53.5|         51.9|                 3.0|                            46.5|                         1.2|\n",
      "|ASALTO|ARMA DE FUEGO|21:00:00 - 23:59:59|2011-11-02|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE| NICARAGUA|   HEREDIA|      heredia|           96888|                 58.9|         57.0|                 3.2|                            41.1|                         1.2|\n",
      "|ASALTO|ARMA DE FUEGO|21:00:00 - 23:59:59|2011-11-02|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|      HOMBRE| NICARAGUA|   HEREDIA|      heredia|          334474|                 57.3|         55.6|                 2.9|                            42.7|                         1.3|\n",
      "|ASALTO|ARMA DE FUEGO|18:00:00 - 20:59:59|2011-10-10|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|       MUJER| NICARAGUA|PUNTARENAS|       quepos|           14677|                 56.4|         54.0|                 4.3|                            43.6|                         1.4|\n",
      "|ASALTO|ARMA DE FUEGO|15:00:00 - 17:59:59|2011-07-22|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|       MUJER|COSTA RICA|  SAN JOSE| desamparados|           19768|                 57.1|         55.0|                 3.7|                            42.9|                         1.3|\n",
      "|ASALTO|ARMA DE FUEGO|15:00:00 - 17:59:59|2011-07-22|EDIFICACION|      CAFÉ| INTERNET [EDIFIC...|Mayor de edad|       MUJER|COSTA RICA|  SAN JOSE| desamparados|           26951|                 59.3|         56.4|                 4.8|                            40.7|                         1.1|\n",
      "+------+-------------+-------------------+----------+-----------+----------+--------------------+-------------+------------+----------+----------+-------------+----------------+---------------------+-------------+--------------------+--------------------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datosIntegrados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"postgresql-42.1.4.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mandaPostgres(dataframe, nombre):\n",
    "    dataframe \\\n",
    "        .write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .mode('overwrite') \\\n",
    "        .option(\"url\", \"jdbc:postgresql://localhost/\") \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"postgres\") \\\n",
    "        .option(\"dbtable\", nombre) \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o123.save.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:105)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:105)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:194)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:198)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8f077e03a4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmandaPostgres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatosIntegrados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Delito\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prueba\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-69806f76fb4e>\u001b[0m in \u001b[0;36mmandaPostgres\u001b[0;34m(dataframe, nombre)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmandaPostgres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnombre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdataframe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o123.save.\n: java.sql.SQLException: No suitable driver\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:105)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:105)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:194)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:198)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "mandaPostgres(datosIntegrados.select(\"Delito\"), \"prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
